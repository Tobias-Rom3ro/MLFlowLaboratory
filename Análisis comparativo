游꿢 Reflexi칩n sobre los resultados del Taller

1. 쮺u치l modelo tuvo menor latencia promedio?
Para responder esto necesitas revisar en MLflow:

Para ML Cl치sico: busca m칠tricas como training_time o prediction_time
Para LLMs: busca m칠tricas como avg_latency, response_time o tokens_per_second

Hip칩tesis esperada:

DeepSeek (v3.1-free) probablemente tiene mayor latencia por ser gratuito
Gemini flash deber칤a ser m치s r치pido (est치 optimizado para velocidad)

2. 쮺u치l gener칩 m치s tokens?
Gemini gener칩 significativamente m치s tokens que DeepSeek

- Gemini: ~3,028 tokens totales (~2,915 de respuesta)
- DeepSeek: ~1,958 tokens totales (~1,845 de respuesta)

Diferencia: Gemini gener칩 aproximadamente 54% m치s tokens que DeepSeek.

Al revisar a profundidad, sabemos que:
**Gemini fue m치s verboso en:**

- Code Generation: 1,450 tokens vs 820 de DeepSeek (77% m치s)
  Gemini incluy칩 explicaciones m치s detalladas del c칩digo
  Agreg칩 m칰ltiples ejemplos de uso


- Question Answering: 975 tokens vs 625 de DeepSeek (56% m치s)
  Respuesta m치s extensa sobre aprendizaje por refuerzo
  Mayor nivel de detalle en las explicaciones


**DeepSeek fue m치s conciso en:**

-Translation: 25 tokens vs 95 de Gemini (73% menos)
  DeepSeek dio una traducci칩n directa
  Gemini ofreci칩 3 opciones alternativas

3. 쯈u칠 par치metros podr칤as optimizar en futuros experimentos?
Para Machine Learning Cl치sico:
Par치metros a explorar:

C: El baseline (C=1.0) funcion칩 mejor. Prueba valores como: 0.5, 1.0, 2.0, 5.0
solver: Mant칠n lbfgs que ya funciona bien
class_weight: Agregar 'balanced' si hay desbalanceo de clases
Validaci칩n cruzada: Usar k-fold para m칠tricas m치s robustas

Nuevas m칠tricas a registrar:

training_time: Tiempo de entrenamiento
prediction_time: Tiempo de inferencia
model_size: Tama침o del modelo guardado

Para LLMs:
Par치metros a explorar:

temperature: Controla creatividad (0.0-1.0)

Bajo (0.2): Respuestas m치s determin칤sticas
Alto (0.8): Respuestas m치s creativas


max_tokens: Limita longitud de respuestas
top_p: Alternativa a temperature para sampling
system_prompt: Optimizar las instrucciones iniciales

Nuevas m칠tricas a registrar:

latency_per_token: Velocidad de generaci칩n
total_cost: Costo por llamada (importante para APIs de pago)
token_efficiency: Ratio de tokens 칰tiles vs total

Modelos adicionales a probar:

Claude (Anthropic)
GPT-4 (OpenAI)
Llama 3 (Meta)
